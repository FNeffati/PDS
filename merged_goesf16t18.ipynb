{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:14:36.768916Z",
     "start_time": "2023-11-28T17:14:36.443061Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:14:52.110287Z",
     "start_time": "2023-11-28T13:14:49.174033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/6cts6h556zzgbx26m_g0p4gw0000gn/T/ipykernel_6517/2651289391.py:2: DtypeWarning: Columns (3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('Data/GOES_17_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('Data/GOES_16_merged_data.csv')\n",
    "df2 = pd.read_csv('Data/GOES_17_merged_data.csv')\n",
    "df3 = pd.read_csv('Data/GOES_18_merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                    time     xrsa_flux     xrsb_flux status flare_class\n0    2017-02-07 00:00:00  2.949822e-08  7.072423e-08    NaN         NaN\n1    2017-02-07 00:01:00  2.716477e-08  6.869706e-08    NaN         NaN\n2    2017-02-07 00:02:00  2.996076e-08  6.933664e-08    NaN         NaN\n3    2017-02-07 00:03:00  3.124979e-08  6.796333e-08    NaN         NaN\n4    2017-02-07 00:04:00  3.095799e-08  6.928261e-08    NaN         NaN\n..                   ...           ...           ...    ...         ...\n995  2017-02-07 16:35:00  4.418819e-09  4.118933e-08    NaN         NaN\n996  2017-02-07 16:36:00  3.437882e-09  3.999427e-08    NaN         NaN\n997  2017-02-07 16:37:00  6.502486e-09  4.040907e-08    NaN         NaN\n998  2017-02-07 16:38:00  5.297591e-09  3.889536e-08    NaN         NaN\n999  2017-02-07 16:39:00  6.182436e-09  3.814912e-08    NaN         NaN\n\n[1000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>xrsa_flux</th>\n      <th>xrsb_flux</th>\n      <th>status</th>\n      <th>flare_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-02-07 00:00:00</td>\n      <td>2.949822e-08</td>\n      <td>7.072423e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-02-07 00:01:00</td>\n      <td>2.716477e-08</td>\n      <td>6.869706e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-02-07 00:02:00</td>\n      <td>2.996076e-08</td>\n      <td>6.933664e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-02-07 00:03:00</td>\n      <td>3.124979e-08</td>\n      <td>6.796333e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-02-07 00:04:00</td>\n      <td>3.095799e-08</td>\n      <td>6.928261e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>2017-02-07 16:35:00</td>\n      <td>4.418819e-09</td>\n      <td>4.118933e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>2017-02-07 16:36:00</td>\n      <td>3.437882e-09</td>\n      <td>3.999427e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>2017-02-07 16:37:00</td>\n      <td>6.502486e-09</td>\n      <td>4.040907e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>2017-02-07 16:38:00</td>\n      <td>5.297591e-09</td>\n      <td>3.889536e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>2017-02-07 16:39:00</td>\n      <td>6.182436e-09</td>\n      <td>3.814912e-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:14:55.815883Z",
     "start_time": "2023-11-28T13:14:55.806839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548372\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(df1, df2, on='time', how='outer')\n",
    "merged_df = pd.merge(merged_df, df3, on='time', how='outer')\n",
    "print(len(merged_df))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:00.613145Z",
     "start_time": "2023-11-28T13:14:56.450204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548160\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df[[\"time\",\"xrsa_flux_x\",\"xrsb_flux_x\",\"status_x\",\"flare_class_x\"]]\n",
    "merged_df = merged_df.rename(columns={\"xrsa_flux_x\": \"xrsa_flux\", \"xrsb_flux_x\": \"xrsb_flux\", 'status_x':'status', 'flare_class_x': 'flare_class'})\n",
    "\n",
    "mask = merged_df['xrsa_flux'] != merged_df['xrsa_flux'].isna\n",
    "\n",
    "merged_df = merged_df[mask]\n",
    "\n",
    "merged_df = merged_df.drop_duplicates(subset='time', keep='first')\n",
    "print(len(merged_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:01.763143Z",
     "start_time": "2023-11-28T13:15:00.615231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[\"time\"].is_unique)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:04.399744Z",
     "start_time": "2023-11-28T13:15:03.525026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Time Values:\n",
      "Empty DataFrame\n",
      "Columns: [time, xrsa_flux, xrsb_flux, status, flare_class]\n",
      "Index: []\n",
      "0\n",
      "3548160\n",
      "Are there duplicate time values? False\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_df[merged_df.duplicated('time', keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(\"Duplicate Time Values:\")\n",
    "print(duplicates)\n",
    "print(len(duplicates))\n",
    "print(len(merged_df))\n",
    "has_duplicates = merged_df['time'].duplicated().any()\n",
    "print(f\"Are there duplicate time values? {has_duplicates}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:08.448821Z",
     "start_time": "2023-11-28T13:15:07.717710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "first_value = merged_df.head(1)\n",
    "first_value_time = str(first_value['time'].item())\n",
    "first_value_time = datetime.strptime(first_value_time, '%Y-%m-%d %H:%M:%S')\n",
    "print(first_value_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:11.077148Z",
     "start_time": "2023-11-28T13:15:11.066393Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-06 23:59:00\n"
     ]
    }
   ],
   "source": [
    "last_value = merged_df.tail(1)\n",
    "last_value_time = str(last_value['time'].item())\n",
    "last_value_time = datetime.strptime(last_value_time, '%Y-%m-%d %H:%M:%S')\n",
    "print(last_value_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:13.770315Z",
     "start_time": "2023-11-28T13:15:13.763151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548159\n"
     ]
    }
   ],
   "source": [
    "time_difference = last_value_time - first_value_time\n",
    "amount_of_minutes = int(time_difference.total_seconds() / 60)\n",
    "print(amount_of_minutes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:14.369429Z",
     "start_time": "2023-11-28T13:15:14.365716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548160\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:15.285107Z",
     "start_time": "2023-11-28T13:15:15.279056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"full_set.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:15:30.544825Z",
     "start_time": "2023-11-28T13:15:25.601424Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transforming the Dataset from long format to wide  Format "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('full_set.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:30:55.743578Z",
     "start_time": "2023-11-28T13:30:53.947968Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "merged_df = merged_df[[\"time\",\"xrsa_flux\",\"status\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:30:55.814939Z",
     "start_time": "2023-11-28T13:30:55.745736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "7573"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_event = False\n",
    "event_label = 0\n",
    "labels = []\n",
    "\n",
    "for status in merged_df['status']:\n",
    "    if status == 'EVENT_PEAK':\n",
    "        event_label = 1\n",
    "    else:\n",
    "        event_label = 0\n",
    "    labels.append(event_label)\n",
    "\n",
    "cleaned = merged_df[[\"time\",\"xrsa_flux\"]]\n",
    "cleaned['Label'] = labels\n",
    "\n",
    "# removing all the 2017 data\n",
    "cleaned['time'] = pd.to_datetime(cleaned['time'])\n",
    "\n",
    "start_time = pd.to_datetime('2018-01-01 00:00:00')\n",
    "cleaned = cleaned[(cleaned['time'] >= start_time)]\n",
    "\n",
    "cleaned = cleaned.dropna(subset=['xrsa_flux'])\n",
    "cleaned = cleaned.reset_index(drop=True)\n",
    "\n",
    "sum(cleaned.Label)\n",
    "\n",
    "# cleaned.head(10000)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:30:57.246605Z",
     "start_time": "2023-11-28T13:30:55.846740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "sum(cleaned.Label)\n",
    "peaks = []\n",
    "non_peaks = []\n",
    "\n",
    "window_size = 50 # will result in a window of size 100 \n",
    "\n",
    "for i  in range(window_size, len(cleaned) - window_size):\n",
    "    window = cleaned.iloc[i- window_size : i+ window_size + 1]\n",
    "    if window.Label[i] == 1:\n",
    "        time = cleaned.iloc[i]['time']  # Capture the time of the peak\n",
    "        row = [time] + list(window['xrsa_flux'].reset_index(drop=True))\n",
    "        row.append(1)  # Label for peak\n",
    "        peaks.append(row)\n",
    "    if window.Label[i] == 0: \n",
    "        time = cleaned.iloc[i]['time']  # Capture the time of the peak\n",
    "        row = [time] + list(window['xrsa_flux'].reset_index(drop=True))\n",
    "        row.append(0)\n",
    "        non_peaks.append(row)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:36:01.487180Z",
     "start_time": "2023-11-28T13:31:05.529684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7573\n",
      "2977318\n"
     ]
    }
   ],
   "source": [
    "print(len(peaks))\n",
    "print(len(non_peaks))\n",
    "\n",
    "mix = peaks + non_peaks[: (len(peaks) * 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:25:31.872514Z",
     "start_time": "2023-11-28T14:25:31.852589Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "random.shuffle(mix)\n",
    "new_wide_series_list = mix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:25:42.058220Z",
     "start_time": "2023-11-28T14:25:42.057096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30292\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(new_wide_series_list))\n",
    "print(type(new_wide_series_list[0]))\n",
    "\n",
    "new_transposed_df = pd.DataFrame(new_wide_series_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:25:42.580094Z",
     "start_time": "2023-11-28T14:25:42.379901Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "new_transposed_df.columns = ['time'] + [str(i) for i in range((window_size * 2) + 1)] + ['Label'] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:26:07.458055Z",
     "start_time": "2023-11-28T14:26:07.446729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "new_transposed_df.to_csv(\"wide_100_timein.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:26:33.416Z",
     "start_time": "2023-11-28T14:26:31.908458Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## converting code to a function that takes input window size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def window_creater(file_path='full_set.csv' ,window_size=50, non_peak_proportion=3):\n",
    "    \"\"\"\n",
    "    :param file_path: default is 'full_set.csv'. It's the full dataset of merged readings from 3 satellites in the long format. \n",
    "    :param window_size: default is 50. Integer How many Readings should sandwich the peak value (center)\n",
    "    :param non_peak_proportion: default is 3. How many non-peaks should we have for every peak  \n",
    "    :return: A dataframe that is in the wide format with columns \"time\", [n amount of readings], \"label\"\n",
    "    \"\"\"\n",
    "    source_data = pd.read_csv(file_path)\n",
    "    source_data = source_data[[\"time\",\"xrsa_flux\",\"status\"]]\n",
    "    labels = []\n",
    "    \n",
    "    for status in source_data['status']:\n",
    "        if status == 'EVENT_PEAK':\n",
    "            event_label = 1\n",
    "        else:\n",
    "            event_label = 0\n",
    "        labels.append(event_label)\n",
    "    \n",
    "    cleaned = source_data[[\"time\",\"xrsa_flux\"]]\n",
    "    cleaned['Label'] = labels\n",
    "    \n",
    "    # removing all the 2017 data\n",
    "    cleaned['time'] = pd.to_datetime(cleaned['time'])\n",
    "    \n",
    "    start_time = pd.to_datetime('2018-01-01 00:00:00')\n",
    "    cleaned = cleaned[(cleaned['time'] >= start_time)]\n",
    "    \n",
    "    cleaned = cleaned.dropna(subset=['xrsa_flux']).reset_index(drop=True)\n",
    "    \n",
    "    peaks = []\n",
    "    non_peaks = []\n",
    "    \n",
    "    for i  in range(window_size, len(cleaned) - window_size):\n",
    "        window = cleaned.iloc[i- window_size : i+ window_size + 1]\n",
    "        if window.Label[i] == 1:\n",
    "            time = cleaned.iloc[i]['time']  # Capture the time of the peak\n",
    "            row = [time] + list(window['xrsa_flux'].reset_index(drop=True))\n",
    "            row.append(1)  # Label for peak\n",
    "            peaks.append(row)\n",
    "        if window.Label[i] == 0: \n",
    "            time = cleaned.iloc[i]['time']  # Capture the time of the peak\n",
    "            row = [time] + list(window['xrsa_flux'].reset_index(drop=True))\n",
    "            row.append(0)\n",
    "            non_peaks.append(row)\n",
    "            \n",
    "    mixed_peaks = peaks + non_peaks[: (len(peaks) * non_peak_proportion)]\n",
    "    random.shuffle(mixed_peaks)\n",
    "\n",
    "    final_transposed_df = pd.DataFrame(mixed_peaks)\n",
    "    final_transposed_df.columns = ['time'] + [str(i) for i in range((window_size * 2) + 1)] + ['Label']\n",
    "    \n",
    "    return final_transposed_df\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:14:44.186978Z",
     "start_time": "2023-11-28T17:14:44.181456Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    22719\n",
      "1     7573\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Testing to hopefully get the same results from earlier \n",
    "output_df = window_creater()\n",
    "counts = output_df['Label'].value_counts()\n",
    "\n",
    "print(counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:19:51.736937Z",
     "start_time": "2023-11-28T17:14:45.745357Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
